#+title: REFINED Big Data Exam Notes 2020
* Subjects in past papers
** TODO Week 10
   - Data Scaling
*** Apache Hadoop
    The unique features of HDFS are:
    - Failure tolerant
    - Scalable - the read/write capacity scales fairly well with the number of data nodes
    - Industry standard - lots of other distributed apps are built on top of HDFS
    - Pairs well with MapReduce
**** NameNode
     is a master server that manages the file system namespace and regulates clients access to
     files. It also executes file system namespace operations like opening, closing and renaming
     files/directories and determins the mapping of blocks to datanodes.
**** DataNode
     Usually for every node in a clustor, there is a datanode, which manages storage attached to
     the nodes that they run on and serves read and write requests from the file system clients
     and perform block creation, deletion and copying upon instruction from the NameNode.

*** Apache Spark
*** Apache Spark vs Hadoop
**** In-Memory Computation
    Spark is based on in-memory computation. It saves and loads data in the form of RAM instead of
    disk. Spark can run tasks 100 times faster using in-memory computations and 10 times faster using
    disk than traditional map reduce tasks.
    In hadoop tasks are distributed among the nodes of a cluster, which in turn save the data on disk.
    When that data is required for processing, each node has to load the data from disk and save the
    data on disk when it's done performing the operation. This adds cost in speed and time Coz disk
    ops are much slower than RAM.
**** Supports Real time and batch processing
     Spark supports batch data processing where a group of transactions is collected over a period of
     time. It also supports real-time data processing, where the data is continuiously flowing from
     the source.
     Hadoop was only designed for batch processing, taking large data sets input at once, processing them
     and writing a large output.
**** Support for Multiple Transformation
     Hadoop only supports Map reduce, where as Spark supports multiple transformations.
   Spark is not a replacement of Hadoop, its designed to run on top of it.
** TODO Week 7/8
*** Supervised learning
     Supervised learning is when you have input variable X and an ouput variable Y and
     you use an algorithm to learn the mapping function from the input to the output.
     y = f(x)
     The goal is to approximate the function so well that when given new input the
     function is able to predict the output variables.
     Supervised learning can be grouped into classification and regression problems.
*** Unsupervised learning
    Unspervised learning is when you have input variable x and no corisponding output.
    The goal is to model the underlying structure of the data in order to learn more
    about the data.
    There are no correct answers, the algorithms are left alone to discover and present
    structures and patterns in the data.
    Unsupervised learning problems can be grouped into Clustering and Association problems.

*** TODO Machine Learning Algorithms
** Week 3
*** Mongo import command
    mongoimport --db exam --collection restaurants --drop -file bd-data.json
**** Why use --drop?
     The drop flag has to be added because if the collection already exist, the file's data
     will be added to it instead of replacing it.
*** Mongo CRUD ops
     db.restaurants.find({"cuisne": "hamburgers", "grades.grade": {$all: ['A', 'B', 'Z']}},
     {"name":1, "grade":1, "_id":0}).pretty();
*** Mongo aggregation pipeline
***** Aggregation pipeline
      The same as piping in linux terminal, "Modeled on the concept of data processing
      pipelines".
      - $match
        Filters the document stream to only allow matching documents based on input condition
        to pass to the next function.
      - $group
        Groups input documents by a specified identifier expression, like an equality.
        Then applies an accumulator expression (second argument) to each group.
        Outputs a new collection with one document per group, the document will only
        contain the identifier field and the accumulated fields.
        - the group syntax MUST have an "_id" property.
        - for aggregation to all fields, use null for the _id
      - example
        db.orders.aggregate( [
                              { $match: { status: "A" } },
                              { $group: { _id: "$cust_id", total: { $sum: "$amount" }}}
        ])
*** Mongo Map-reduce
***** Map-reduce
      A MapReduce program is composed of a map procedure (or method), which performs filtering and
      sorting (such as sorting students by first name into queues, one queue for each name, like a
      bunch of Gavins, Jacks etc..), and a reduce method, which performs a summary operation
      (such as counting the number of students in each queue, yielding name frequencies).
****** Stages of MapReduce
       1) Query to select data, this is only needed if the function is not being perfomed on all data.
       2) Map function returns new collection with a key and the values of the selected fields
       3) The Reduce function is some form of accumulator function to give a total of the fields
       4) The results are output into a new collection, specified as the "out" property value
****** Example
       - Create a map function. First arg is the group by key, second arg is the values. This can
         also be though of as "what do you want an array of?", which means it can have an int of 1
         passed in which is used to count the amount of documents. This is handy when there is no
         key to be gathering the values of in each document, like building a price array for each
         transaction user document.
       var mapFunction1 = function() {
                       emit(this.cust_id, this.price);
                   };
       - Create reducer function to in this case "sum" all the values. Second arg is the array
         from the map function.
         var reduceFunction1 = function(keyCustId, valuesPrices) {
                          return Array.sum(valuesPrices);
                      };

       - Perform the operation. The { out: "..."} is the name the new collection the results will be placed in.
         db.orders.mapReduce(
                     mapFunction1,
                     reduceFunction1,
                     {
                       query: { "price": {gt: 500}}, // optional query to initial data selection
                       out: "map_reduce_example"
                     }
                   )

*** Mongo MapReduce vs aggregation pipeline
    - For most aggregation operations, the aggregation pipeline provides better performance and a
      more coherant interface. However map reduce provides some flexability that aggregation pipeline
      does not.
    - Map reduce has high overhead, with operations on even small datasets taking 100s of miliseconds,
      making aggregation pipeline likely to be faster on small datasets.
    - Map reduce is likely to be faster on large datasets as as its operations can be run in parallel.
    - Because map reduce uses javascript functions, it has full access to the language which aggregation
      pipeline does not, potentially giving map reduce more power/ability.
*** Mongo python
**** Example Code
     from pymongo import MongoClient
     client = MongoClient()
     companies =client.exam. companies
     for company in companies.find( {"tag_list" : {$in: ["wiki", "media‐industry"},
     "total_money_raise": {$gt: 500000}  ):
         print company [‘name’] + ” ” + company [‘homepage_url‘] + “ “ + company
         [‘offices.country_code’]
**** Code Breakdown
    - import the MongoDB python library module
    - Create a reference variable to the running mongod instance, it will connect to default host/port
    - create a reference variable and assign it to the companies collection from exam database.
    - This script will find all the companies with ‘tag_list’ including ‘wiki’ or ‘media‐inductry’, and
      ‘total_money_raise’ greater than 500000, and print out the ‘name’, ‘homepage_id’ and
      ‘country_code’ of the companies.

*** Mongo vs SQL
    Mongo is better for:
    - schemas that can be changed over time
    - when no schema is required
    - unstructured or semi-structured data
    - has great scalability
    - speed
    Sql is better for:
    - Complex transactions
    - Joins are requied
    - fixed schema
    - more secure
** Week 5
*** Dimentionality Reduction
    Dimentionality Reduction refers to the process of converting a data set that has vast
    dimentions into data with lesser dimensions ensuring that it conveys similar information
    correctly.
    Dimensions of data set N can be reduced to K (K < N). The K dimensions can be a comination
    of dimensions or new dimensions that represent existing multiple ones.
    The benefits are:
    - Aids data compression and storage space reduction
    - Improved performance
    - Takes care of multi-colliniarity improving model performance
    - removes redundent features
    Examples:
    - Dropping variables with too many missing values
    - Dropping variables with low variance (similar shit)
    - Use of methods such as Correlations, Decision Trees
